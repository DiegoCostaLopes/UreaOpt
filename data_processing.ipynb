{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script creates the database only with production columns, more focused in my work.\n",
    "crops = ['cocoa', 'coffee', 'corn', 'rice', 'soy', 'sugarcane']\n",
    "start_cols = ['name', 'state', 'location_type']\n",
    "data_pam = pd.DataFrame(columns=start_cols)\n",
    "\n",
    "for crop in crops:\n",
    "\n",
    "    rename_cols = {\n",
    "        'Local': 'name',\n",
    "        'Quantidade produzida (Toneladas)': crop,\n",
    "        'Área destinada à colheita (Hectares)' : 'area_planted_' + crop,\n",
    "        'Área plantada (Hectares)': 'area_planted_' + crop,\n",
    "        'Área colhida (Hectares)': 'area_harvested_' + crop,\n",
    "        'UF': 'state',\n",
    "        'Tipo região': 'location_type',\n",
    "    }\n",
    "\n",
    "    new_cols = list(set(rename_cols.values()))\n",
    "\n",
    "    df_temp = pd.read_excel('data/processed/' + crop + '.xlsx').rename(columns=rename_cols)\n",
    "    df_temp = df_temp[new_cols].drop_duplicates()\n",
    "    data_pam = data_pam.merge(df_temp, on=['name', 'state', 'location_type'], how='outer')\n",
    "    \n",
    "# cleaning strings up\n",
    "\n",
    "data_pam['name'] = data_pam['name'].apply(unidecode)\n",
    "data_pam['state'] = data_pam['state'].apply(unidecode)\n",
    "data_pam['location_type'] = (data_pam['location_type'].apply(unidecode).str\n",
    "                       .replace('Municipio', 'city')\n",
    "                       .replace('Microrregiao', 'microregion')\n",
    "                       .replace('Mesorregiao', 'macroregion')\n",
    "                       .replace('UF', 'state'))\n",
    "# putting all productions int64o the same df\n",
    "\n",
    "\n",
    "data_pam.to_csv('data/processed/data_pam_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rename_cols = {\n",
    "    'Nome município': 'name',\n",
    "    'Código IBGE município': 'id_city',\n",
    "    'Nome microrregião': 'microregion',\n",
    "    'Código IBGE microrregião': 'id_microregion',\n",
    "    'Nome mesorregião': 'macroregion',\n",
    "    'Código IBGE mesorregião': 'id_macroregion',\n",
    "    'Nome UF': 'state',\n",
    "    'Código IBGE UF': 'id_state',\n",
    "    'Sigla UF': 'UF',\n",
    "    'Nome região': 'region'\n",
    "}\n",
    "data_ibge_city = pd.read_excel('data/raw/ibge_codes/municipios.xlsx').rename(columns=rename_cols)\n",
    "data_ibge_city['location_type'] = 'city'\n",
    "data_ibge_city['id'] = data_ibge_city['id_city'].astype('int64')\n",
    "str_columns = ['name', 'microregion','macroregion','state','UF','region']\n",
    "for col in str_columns:\n",
    "    data_ibge_city[col] = data_ibge_city[col].apply(unidecode)\n",
    "    \n",
    "data_ibge_microregion = pd.read_excel('data/raw/ibge_codes/microrregiao.xlsx').rename(columns=rename_cols)\n",
    "data_ibge_microregion['name'] = data_ibge_microregion['microregion']\n",
    "data_ibge_microregion['location_type'] = 'microregion'\n",
    "data_ibge_microregion['id'] = data_ibge_microregion['id_microregion'].astype('int64')\n",
    "str_columns = ['name', 'microregion', 'macroregion','state','UF','region']\n",
    "\n",
    "for col in str_columns:\n",
    "    data_ibge_microregion[col] = data_ibge_microregion[col].apply(unidecode)\n",
    "\n",
    "data_ibge_macroregion = pd.read_excel('data/raw/ibge_codes/mesorregiao.xlsx').rename(columns=rename_cols)\n",
    "data_ibge_macroregion['name'] = data_ibge_macroregion['macroregion']\n",
    "data_ibge_macroregion['location_type'] = 'macroregion'\n",
    "data_ibge_macroregion['id'] = data_ibge_macroregion['id_macroregion'].astype('int64')\n",
    "str_columns = ['name','state', 'macroregion', 'UF','region']\n",
    "\n",
    "for col in str_columns:\n",
    "    data_ibge_macroregion[col] = data_ibge_macroregion[col].apply(unidecode)\n",
    "\n",
    "data_ibge_state = pd.read_excel('data/raw/ibge_codes/uf.xlsx').rename(columns=rename_cols)\n",
    "data_ibge_state['name'] = data_ibge_state['state']\n",
    "data_ibge_state['location_type'] = 'state'\n",
    "data_ibge_state['id'] = data_ibge_state['id_state'].astype('int64')\n",
    "str_columns = ['name', 'state', 'UF','region']\n",
    "\n",
    "for col in str_columns:\n",
    "    data_ibge_state[col] = data_ibge_state[col].apply(unidecode)\n",
    "\n",
    "data_ibge = pd.concat([data_ibge_city, data_ibge_microregion, data_ibge_macroregion, data_ibge_state])\n",
    "\n",
    "data_ibge.to_csv('data/processed/data_ibge.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_pam.merge(data_ibge, on=['name', 'location_type', 'state'], how='inner')\n",
    "crops = ['cocoa', 'coffee', 'corn', 'rice', 'soy', 'sugarcane']\n",
    "df['total'] = df[crops].sum(axis=1)\n",
    "df = df.set_index('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out urea consumption of each region. At first we'll be considering only the selected crops.\n",
    "# source - IFA report 2018\n",
    "kgN_kgurea = 0.46\n",
    "\n",
    "N_consumption = {\n",
    "    'rice': 83,\n",
    "    'corn': 68,\n",
    "    'soy': 16,\n",
    "    'sugarcane': 76,\n",
    "    'coffee': 161,\n",
    "    'cocoa' : 49\n",
    "}\n",
    "\n",
    "df['urea_demand'] = 0\n",
    "for crop in crops:\n",
    "    df['urea_demand'] = df['urea_demand'] + N_consumption[crop] * df['area_planted_' + crop] / kgN_kgurea / 1000\n",
    "\n",
    "# biomass residue to crop yield. Source - de Souza, 2021\n",
    "\n",
    "residue_data = {\n",
    "    'rice_husk': {'origin': 'rice', 'ratio': 1.55 * 0.4, 'cost': 60},\n",
    "    'corn_stover': {'origin': 'corn', 'ratio': 1.68 * 0.4, 'cost': 60},\n",
    "    'soy_straw' : {'origin': 'soy', 'ratio': 2.3 * 0.3, 'cost': 60},\n",
    "    'sugarcane_straw': {'origin': 'sugarcane', 'ratio': 0.22 * 0.59, 'cost': 60},\n",
    "    'sugarcane_bagasse': {'origin': 'sugarcane', 'ratio': 0.22 * 0.25, 'cost': 40},\n",
    "    'coffee_husk':  {'origin': 'coffee', 'ratio': 0.59 * 0.50, 'cost': 60},\n",
    "    'cocoa_residue': {'origin': 'cocoa', 'ratio': 0.59 * 0.50, 'cost': 60}, # sem dados\n",
    "}\n",
    "\n",
    "for residue in residue_data:\n",
    "    df[residue] = df[residue_data[residue]['origin']] * residue_data[residue]['ratio']\n",
    "    df[residue+'_cost'] = residue_data[residue]['cost']\n",
    "\n",
    "df['urea_price'] = 310\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each microregion, finding out what is the city with maximum production. that will be the reference city for distance purposes.\n",
    "import numpy as np\n",
    "microregions = df['microregion'].unique()\n",
    "\n",
    "df.loc[df['location_type'] == 'city', 'location_id'] = df.loc[df['location_type'] == 'city'].index\n",
    "\n",
    "for microregion in microregions:\n",
    "    if microregion is not np.nan:\n",
    "        df.loc[(df['microregion'] == microregion) & (df['location_type'] == 'microregion'), 'location_id'] = df['total'].loc[(df['location_type'] == 'city') & (df['microregion'] == microregion)].idxmax()\n",
    "\n",
    "macroregions = df['macroregion'].unique()\n",
    "\n",
    "for macroregion in macroregions:\n",
    "    if macroregion is not np.nan:\n",
    "        df.loc[(df['macroregion'] == macroregion) & (df['location_type'] == 'macroregion'), 'location_id'] = df['total'].loc[(df['location_type'] == 'city') & (df['macroregion'] == macroregion)].idxmax()\n",
    "\n",
    "states = df['state'].unique()\n",
    "for state in states:\n",
    "    if state is not np.nan:\n",
    "        df.loc[(df['state'] == state) & (df['location_type'] == 'state'), 'location_id'] = df['total'].loc[(df['location_type'] == 'city') & (df['state'] == state)].idxmax()\n",
    "\n",
    "df['location_id'] = df['location_id'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = pd.read_csv('data/raw/distance_matrix/matriz_distancias.csv', index_col=['origem', 'destino']).drop(columns='tempo')['distancia'].unstack()\n",
    "distance_matrix.index.name = 'origin'\n",
    "\n",
    "correct_keys = list(df.index)\n",
    "wrong_keys = (np.array(list(df.index)) / 10).astype(int)\n",
    "rename_dict = {i: j for i, j in zip (wrong_keys, correct_keys)}\n",
    "distance_matrix = distance_matrix.rename(index=rename_dict, columns=rename_dict)\n",
    "distance_matrix = distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting final version of both the location database and the distance matrix\n",
    "\n",
    "df.to_pickle('data/processed/location_db.p')\n",
    "distance_matrix.to_pickle('data/processed/distance_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sp = df.loc[(df['state'] == 'Sao Paulo') & (df['location_type'] == 'city')]\n",
    "code_list = data_sp['location_id'].to_numpy()\n",
    "new_dm = distance_matrix.loc[code_list, code_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
