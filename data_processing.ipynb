{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script creates the database only with production columns, more focused in my work.\n",
    "crops = ['cocoa', 'coffee', 'corn', 'rice', 'soy', 'sugarcane']\n",
    "start_cols = ['name', 'state', 'location_type']\n",
    "data_pam = pd.DataFrame(columns=start_cols)\n",
    "\n",
    "for crop in crops:\n",
    "\n",
    "    rename_cols = {\n",
    "        'Local': 'name',\n",
    "        'Quantidade produzida (Toneladas)': crop,\n",
    "        'Área destinada à colheita (Hectares)' : 'area_planted_' + crop,\n",
    "        'Área plantada (Hectares)': 'area_planted_' + crop,\n",
    "        'Área colhida (Hectares)': 'area_harvested_' + crop,\n",
    "        'UF': 'state',\n",
    "        'Tipo região': 'location_type',\n",
    "    }\n",
    "\n",
    "    new_cols = list(set(rename_cols.values()))\n",
    "\n",
    "    df_temp = pd.read_excel('data/processed/' + crop + '.xlsx').rename(columns=rename_cols)\n",
    "    df_temp = df_temp[new_cols].drop_duplicates()\n",
    "    data_pam = data_pam.merge(df_temp, on=['name', 'state', 'location_type'], how='outer')\n",
    "    \n",
    "# cleaning strings up\n",
    "\n",
    "data_pam['name'] = data_pam['name'].apply(unidecode)\n",
    "data_pam['state'] = data_pam['state'].apply(unidecode)\n",
    "data_pam['location_type'] = (data_pam['location_type'].apply(unidecode).str\n",
    "                       .replace('Municipio', 'city')\n",
    "                       .replace('Microrregiao', 'microregion')\n",
    "                       .replace('Mesorregiao', 'macroregion')\n",
    "                       .replace('UF', 'state'))\n",
    "# putting all productions int64o the same df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rename_cols = {\n",
    "    'Nome município': 'name',\n",
    "    'Código IBGE município': 'id_city',\n",
    "    'Nome microrregião': 'microregion',\n",
    "    'Código IBGE microrregião': 'id_microregion',\n",
    "    'Nome mesorregião': 'macroregion',\n",
    "    'Código IBGE mesorregião': 'id_macroregion',\n",
    "    'Nome UF': 'state',\n",
    "    'Código IBGE UF': 'id_state',\n",
    "    'Sigla UF': 'UF',\n",
    "    'Nome região': 'region'\n",
    "}\n",
    "\n",
    "col_dtypes = {\n",
    "    'id': 'int64',\n",
    "    'id_city': 'int64',\n",
    "    'id_microregion': 'int64',\n",
    "    'id_macroregion': 'int64',\n",
    "    'id_state': 'int64',\n",
    "}\n",
    "\n",
    "data_ibge_city = pd.read_excel('data/raw/ibge_codes/municipios.xlsx').rename(columns=rename_cols)\n",
    "data_ibge_city['location_type'] = 'city'\n",
    "\n",
    "data_ibge_city = data_ibge_city.convert_dtypes(col_dtypes)\n",
    "data_ibge_city['id'] = data_ibge_city['id_city'] \n",
    "str_columns = ['name', 'microregion','macroregion','state','UF','region']\n",
    "for col in str_columns:\n",
    "    data_ibge_city[col] = data_ibge_city[col].apply(unidecode)\n",
    "    \n",
    "data_ibge_microregion = pd.read_excel('data/raw/ibge_codes/microrregiao.xlsx').rename(columns=rename_cols)\n",
    "data_ibge_microregion['name'] = data_ibge_microregion['microregion']\n",
    "data_ibge_microregion['location_type'] = 'microregion'\n",
    "data_ibge_microregion = data_ibge_microregion.convert_dtypes(col_dtypes)\n",
    "data_ibge_microregion['id'] = data_ibge_microregion['id_microregion']\n",
    "str_columns = ['name', 'microregion', 'macroregion','state','UF','region']\n",
    "for col in str_columns:\n",
    "    data_ibge_microregion[col] = data_ibge_microregion[col].apply(unidecode)\n",
    "\n",
    "data_ibge_macroregion = pd.read_excel('data/raw/ibge_codes/mesorregiao.xlsx').rename(columns=rename_cols)\n",
    "data_ibge_macroregion['name'] = data_ibge_macroregion['macroregion']\n",
    "data_ibge_macroregion['location_type'] = 'macroregion'\n",
    "data_ibge_macroregion = data_ibge_macroregion.convert_dtypes(col_dtypes)\n",
    "data_ibge_macroregion['id'] = data_ibge_macroregion['id_macroregion']\n",
    "str_columns = ['name','state', 'macroregion', 'UF','region']\n",
    "\n",
    "for col in str_columns:\n",
    "    data_ibge_macroregion[col] = data_ibge_macroregion[col].apply(unidecode)\n",
    "\n",
    "data_ibge_state = pd.read_excel('data/raw/ibge_codes/uf.xlsx').rename(columns=rename_cols)\n",
    "data_ibge_state['name'] = data_ibge_state['state']\n",
    "data_ibge_state['location_type'] = 'state'\n",
    "data_ibge_state = data_ibge_state.convert_dtypes(col_dtypes)\n",
    "data_ibge_state['id'] = data_ibge_state['id_state']\n",
    "str_columns = ['name', 'state', 'UF','region']\n",
    "\n",
    "for col in str_columns:\n",
    "    data_ibge_state[col] = data_ibge_state[col].apply(unidecode)\n",
    "\n",
    "data_ibge = pd.concat([data_ibge_city, data_ibge_microregion, data_ibge_macroregion, data_ibge_state])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_pam.merge(data_ibge, on=['name', 'location_type', 'state'], how='inner')\n",
    "crops = ['cocoa', 'coffee', 'corn', 'rice', 'soy', 'sugarcane']\n",
    "df['total'] = df[crops].sum(axis=1)\n",
    "df = df.set_index('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each microregion, finding out what is the city with maximum production. that will be the reference city for distance purposes.\n",
    "df.loc[df['location_type'] == 'city', 'location_id'] = df.loc[df['location_type'] == 'city'].index\n",
    "\n",
    "microregions = df.loc[df['location_type'] == 'microregion'].index\n",
    "for microregion in microregions:\n",
    "    df.loc[microregion, 'location_id'] = df['total'].loc[(df['location_type'] == 'city') & (df['id_microregion'] == microregion)].idxmax()\n",
    "\n",
    "\n",
    "macroregions = df.loc[df['location_type'] == 'macroregion'].index\n",
    "for macroregion in macroregions:\n",
    "    df.loc[macroregion, 'location_id'] = df['total'].loc[(df['location_type'] == 'city') & (df['id_macroregion'] == macroregion)].idxmax()\n",
    "\n",
    "states = df.loc[df['location_type'] == 'state'].index\n",
    "for state in states:\n",
    "    df.loc[state, 'location_id'] = df['total'].loc[(df['location_type'] == 'city') & (df['id_state'] == state)].idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63394/4004704321.py:57: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['grid_price'] = df['grid_price'].fillna(value=df['average_state_grid_prices'].astype(float))\n"
     ]
    }
   ],
   "source": [
    "# finding out urea consumption of each region. At first we'll be considering only the selected crops.\n",
    "# source - IFA report 2018\n",
    "kgN_kgurea = 0.46\n",
    "\n",
    "N_consumption = {\n",
    "    'rice': 83,\n",
    "    'corn': 68,\n",
    "    'soy': 16,\n",
    "    'sugarcane': 76,\n",
    "    'coffee': 161,\n",
    "    'cocoa' : 49\n",
    "}\n",
    "\n",
    "df['urea_demand'] = 0\n",
    "for crop in crops:\n",
    "    df['urea_demand'] = df['urea_demand'] + N_consumption[crop] * df['area_planted_' + crop] / kgN_kgurea / 1000\n",
    "\n",
    "# biomass residue to crop yield. Source - de Souza, 2021\n",
    "\n",
    "residue_data = {\n",
    "    'rice_husk': {'origin': 'rice', 'ratio': 1.55 * 0.4, 'price': 60},\n",
    "    'corn_stover': {'origin': 'corn', 'ratio': 1.68 * 0.4, 'price': 60},\n",
    "    'soy_straw' : {'origin': 'soy', 'ratio': 2.3 * 0.3, 'price': 60},\n",
    "    'sugarcane_straw': {'origin': 'sugarcane', 'ratio': 0.22 * 0.59, 'price': 60},\n",
    "    'sugarcane_bagasse': {'origin': 'sugarcane', 'ratio': 0.22 * 0.25, 'price': 40},\n",
    "    'coffee_husk':  {'origin': 'coffee', 'ratio': 0.59 * 0.50, 'price': 60},\n",
    "    'cocoa_residue': {'origin': 'cocoa', 'ratio': 0.59 * 0.50, 'price': 60}, # sem dados\n",
    "}\n",
    "biomasses = ['rice_husk', 'corn_stover', 'soy_straw', 'sugarcane_straw', 'sugarcane_bagasse', 'coffee_husk', 'cocoa_residue']\n",
    "residue_data = pd.DataFrame({\n",
    "    'biomass': biomasses,\n",
    "    'origin': ['rice', 'corn', 'soy', 'sugarcane', 'sugarcane', 'coffee', 'cocoa'],\n",
    "    'ratio': [1.55*0.4, 1.68*0.4, 2.3*0.3, .22*0.59, 0.22*0.25, 0.59*0.50, 0.59*0.50],\n",
    "    'HHV_daf': [18.61, 20.50, 18.23, 17.43, 18.56, 18.79, 17.38] ,\n",
    "    'moisture': [0.20, 0.20, 0.20, 0.20, 0.40, 0.20, 0.20]\n",
    "}).set_index('biomass')\n",
    "\n",
    "residue_data['HHV'] = residue_data['HHV_daf'] * (1 - residue_data['moisture'])\n",
    "\n",
    "reference_price_per_GJ = 4.50\n",
    "\n",
    "for biomass in biomasses:\n",
    "    df[biomass] = df[residue_data.loc[biomass, 'origin']] * residue_data.loc[biomass, 'ratio']\n",
    "    df[biomass+'_price'] = reference_price_per_GJ * residue_data.loc[biomass, 'HHV']\n",
    "\n",
    "df['urea_price'] = 380\n",
    "\n",
    "# with the location ids set up, I can assign a power price to each location based on it.\n",
    "df_power = pd.read_pickle('data/processed/power_grid_prices.p').reset_index().rename(columns={\n",
    "    'friendly_name': 'dist_name',\n",
    "    'id_city': 'location_id',\n",
    "}).drop(columns='UF')\n",
    "\n",
    "df = df.reset_index().merge(df_power, on='location_id', how='left', validate='many_to_one').set_index('id')\n",
    "# whatever data we dont have on grid prices will be assumed to be the average of the state.\n",
    "df['average_state_grid_prices'] = df.groupby('state')['grid_price'].transform('mean')\n",
    "df['grid_price'] = df['grid_price'].fillna(value=df['average_state_grid_prices'].astype(float))\n",
    "df = df.rename(columns={'grid_price': 'power_grid_price'})\n",
    "\n",
    "# grid has infinite supply\n",
    "df['power_grid'] = 99999999\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = pd.read_csv('data/raw/distance_matrix/matriz_distancias.csv', index_col=['origem', 'destino']).drop(columns='tempo')['distancia'].unstack()\n",
    "distance_matrix.index.name = 'origin'\n",
    "\n",
    "correct_keys = list(df.index)\n",
    "wrong_keys = (np.array(list(df.index)) / 10).astype(int)\n",
    "rename_dict = {i: j for i, j in zip (wrong_keys, correct_keys)}\n",
    "distance_matrix = distance_matrix.rename(index=rename_dict, columns=rename_dict)\n",
    "distance_matrix = distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting final version of both the main database and the distance matrix\n",
    "df.to_pickle('data/processed/location_db.p')\n",
    "distance_matrix.to_pickle('data/processed/distance_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ureaopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
